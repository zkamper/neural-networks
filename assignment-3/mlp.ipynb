{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, cost: 0.4154090344430028\n",
      "Epoch 2/100, cost: 0.19537300244234115\n",
      "Epoch 3/100, cost: 0.15298225320881303\n",
      "Epoch 4/100, cost: 0.12630018410151284\n",
      "Epoch 5/100, cost: 0.11163912605631957\n",
      "Epoch 6/100, cost: 0.0991762996285956\n",
      "Epoch 7/100, cost: 0.09265037329761407\n",
      "Epoch 8/100, cost: 0.08480669221694213\n",
      "Epoch 9/100, cost: 0.0809530737024147\n",
      "Epoch 10/100, cost: 0.07529280888471443\n",
      "Epoch 11/100, cost: 0.07231001088013939\n",
      "Epoch 12/100, cost: 0.06878193680318694\n",
      "Epoch 13/100, cost: 0.06352399121381488\n",
      "Epoch 14/100, cost: 0.061853693727707826\n",
      "Epoch 15/100, cost: 0.05792026845907587\n",
      "Epoch 16/100, cost: 0.056155479887799065\n",
      "Epoch 17/100, cost: 0.054166103574718294\n",
      "Epoch 18/100, cost: 0.05140212636822651\n",
      "Epoch 19/100, cost: 0.04934253485842886\n",
      "Epoch 20/100, cost: 0.049056180308356075\n",
      "Epoch 21/100, cost: 0.048301739533027264\n",
      "Epoch 22/100, cost: 0.04573088413703572\n",
      "Epoch 23/100, cost: 0.04550583087225497\n",
      "Epoch 24/100, cost: 0.041743404793052497\n",
      "Epoch 25/100, cost: 0.042125938249615406\n",
      "Epoch 26/100, cost: 0.04107278902490375\n",
      "Epoch 27/100, cost: 0.03891599748718591\n",
      "Epoch 28/100, cost: 0.0394158650816389\n",
      "Epoch 29/100, cost: 0.03786201344384551\n",
      "Epoch 30/100, cost: 0.03688075317604827\n",
      "Epoch 31/100, cost: 0.03636832294422318\n",
      "Epoch 32/100, cost: 0.03444868493865207\n",
      "Epoch 33/100, cost: 0.034468850583827565\n",
      "Epoch 34/100, cost: 0.03428346446966588\n",
      "Epoch 35/100, cost: 0.03233938101561316\n",
      "Epoch 36/100, cost: 0.03292112516188005\n",
      "Epoch 37/100, cost: 0.03255698077326377\n",
      "Epoch 38/100, cost: 0.03042964673340713\n",
      "Epoch 39/100, cost: 0.03071405167989743\n",
      "Epoch 40/100, cost: 0.029364333101054914\n",
      "Epoch 41/100, cost: 0.029299971401242124\n",
      "Epoch 42/100, cost: 0.02897301457688197\n",
      "Epoch 43/100, cost: 0.028795584212613714\n",
      "Epoch 44/100, cost: 0.02845807276290865\n",
      "Epoch 45/100, cost: 0.027412485062010036\n",
      "Epoch 46/100, cost: 0.02813902905141768\n",
      "Epoch 47/100, cost: 0.026967446035599855\n",
      "Epoch 48/100, cost: 0.027703484102209016\n",
      "Epoch 49/100, cost: 0.026134006796248336\n",
      "Epoch 50/100, cost: 0.02655494907690321\n",
      "Epoch 51/100, cost: 0.022463474709755606\n",
      "Epoch 52/100, cost: 0.02213971401767876\n",
      "Epoch 53/100, cost: 0.02213662511603225\n",
      "Epoch 54/100, cost: 0.021356140489089757\n",
      "Epoch 55/100, cost: 0.021300207158566228\n",
      "Epoch 56/100, cost: 0.021821240208797898\n",
      "Epoch 57/100, cost: 0.020035581328621897\n",
      "Epoch 58/100, cost: 0.020188053907534163\n",
      "Epoch 59/100, cost: 0.020319523061231953\n",
      "Epoch 60/100, cost: 0.020955955607713284\n",
      "Epoch 61/100, cost: 0.02051678408558156\n",
      "Epoch 62/100, cost: 0.020805335314116564\n",
      "Epoch 63/100, cost: 0.02091508287244238\n",
      "Epoch 64/100, cost: 0.020214390513233236\n",
      "Epoch 65/100, cost: 0.021375099712629922\n",
      "Epoch 66/100, cost: 0.01956386290687391\n",
      "Epoch 67/100, cost: 0.019294245739000467\n",
      "Epoch 68/100, cost: 0.019416290573176672\n",
      "Epoch 69/100, cost: 0.020056254116035798\n",
      "Epoch 70/100, cost: 0.02027421988955581\n",
      "Epoch 71/100, cost: 0.019879412704824777\n",
      "Epoch 72/100, cost: 0.01971708075336946\n",
      "Epoch 73/100, cost: 0.018191108518448518\n",
      "Epoch 74/100, cost: 0.018550402186490383\n",
      "Epoch 75/100, cost: 0.01973008484896347\n",
      "Epoch 76/100, cost: 0.019058800685311426\n",
      "Epoch 77/100, cost: 0.01794261106899253\n",
      "Epoch 78/100, cost: 0.018270706096620324\n",
      "Epoch 79/100, cost: 0.01819100690200163\n",
      "Epoch 80/100, cost: 0.017773549058823404\n",
      "Epoch 81/100, cost: 0.01818414822115346\n",
      "Epoch 82/100, cost: 0.01814735444732725\n",
      "Epoch 83/100, cost: 0.016925084784650598\n",
      "Epoch 84/100, cost: 0.01799427101949457\n",
      "Epoch 85/100, cost: 0.01804450086586564\n",
      "Epoch 86/100, cost: 0.017478691861514316\n",
      "Epoch 87/100, cost: 0.01783109526522089\n",
      "Epoch 88/100, cost: 0.018369335706165932\n",
      "Epoch 89/100, cost: 0.017609335173469614\n",
      "Epoch 90/100, cost: 0.017753981965462136\n",
      "Epoch 91/100, cost: 0.01745380075669484\n",
      "Epoch 92/100, cost: 0.01669636264520075\n",
      "Epoch 93/100, cost: 0.017220868470071336\n",
      "Epoch 94/100, cost: 0.01755891402910859\n",
      "Epoch 95/100, cost: 0.01851184322726909\n",
      "Epoch 96/100, cost: 0.017093457233778513\n",
      "Epoch 97/100, cost: 0.016802474070794365\n",
      "Epoch 98/100, cost: 0.016891615838508674\n",
      "Epoch 99/100, cost: 0.016810612612838276\n",
      "Epoch 100/100, cost: 0.017615635109008824\n",
      "Training took 101.30814385414124 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "data = np.load('mnist_data.npz')\n",
    "train_X = data['train_X']\n",
    "train_Y = data['train_Y']\n",
    "\n",
    "def split_data(data, batch_size):\n",
    "    return [data[i:i + batch_size] for i in range(0, len(data), batch_size)]\n",
    "\n",
    "def softmax(y: np.ndarray) -> np.ndarray:\n",
    "    exp_y = np.exp(y - np.max(y, axis=1, keepdims=True))\n",
    "    return exp_y / np.sum(exp_y, axis=1, keepdims=True)\n",
    "\n",
    "def classify(y: np.ndarray, label: np.ndarray):\n",
    "    return np.sum(np.argmax(y, axis=1) == np.argmax(label, axis=1))\n",
    "\n",
    "def cross_entropy(y: np.ndarray, label: np.ndarray):\n",
    "    return np.mean(-np.sum(label * np.log(y), axis=1))\n",
    "\n",
    "def sigmoid(x: np.ndarray):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x: np.ndarray):\n",
    "    sig = sigmoid(x)\n",
    "    return sig * (1 - sig)\n",
    "\n",
    "def forward_prop(x: np.array, w_1: np.array, b_1: np.array, w_2: np.array, b_2: np.array):\n",
    "    global DROPOUT_RATE\n",
    "    z_1 = x.dot(w_1) + b_1\n",
    "    a_1 = sigmoid(z_1)\n",
    "    # dropout pe hidden layer\n",
    "    a_1 = np.where(np.random.rand(*a_1.shape) < DROPOUT_RATE, 0, a_1)\n",
    "    z_2 = a_1.dot(w_2) + b_2\n",
    "    y = softmax(z_2)\n",
    "    return y, z_1, a_1\n",
    "\n",
    "def back_prop(x: np.array, y: np.array, label: np.array, w_1: np.array, b_1: np.array, w_2: np.array, b_2: np.array, z_1: np.array, a_1: np.array):\n",
    "    global ALPHA\n",
    "    error_out = y - label\n",
    "    error_hidden = error_out.dot(w_2.T) * sigmoid_derivative(z_1)\n",
    "\n",
    "    grad_w_2 = a_1.T.dot(error_out)\n",
    "    grad_b_2 = np.mean(error_out, axis=0)\n",
    "    grad_w_1 = x.T.dot(error_hidden)\n",
    "    grad_b_1 = np.mean(error_hidden, axis=0)\n",
    "\n",
    "    w_2 -= ALPHA * grad_w_2\n",
    "    b_2 -= ALPHA * grad_b_2\n",
    "    w_1 -= ALPHA * grad_w_1\n",
    "    b_1 -= ALPHA * grad_b_1\n",
    "\n",
    "\n",
    "def train_mini_batch(x: np.array, label: np.array):\n",
    "    global w_1, b_1, w_2, b_2\n",
    "    y, z_1, a_1 = forward_prop(x, w_1, b_1, w_2, b_2)\n",
    "    back_prop(x, y, label, w_1, b_1, w_2, b_2, z_1, a_1)\n",
    "    return cross_entropy(y, label)\n",
    "\n",
    "def train(dataset):\n",
    "    global BATCH_SIZE\n",
    "\n",
    "    batches = split_data(dataset, BATCH_SIZE)\n",
    "\n",
    "    cost = 0\n",
    "    for batch in batches:\n",
    "        x = np.array([data[0] for data in batch])\n",
    "        label = np.array([data[1] for data in batch])\n",
    "        cost += train_mini_batch(x, label)\n",
    "    return cost / len(batches)\n",
    "\n",
    "w_1 = np.random.randn(784, 100) * 0.01\n",
    "b_1 = np.zeros(100)\n",
    "w_2 = np.random.randn(100, 10) * 0.01\n",
    "b_2 = np.zeros(10)\n",
    "\n",
    "ALPHA = 0.02\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 50\n",
    "DROPOUT_RATE = 0.15\n",
    "\n",
    "train_data = list(zip(train_X, train_Y))\n",
    "train_data_len = len(train_data)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# TODO: codul propriu zis\n",
    "for i in range(EPOCHS):\n",
    "    if i == 50:\n",
    "        ALPHA = 0.01\n",
    "    np.random.shuffle(train_data)\n",
    "    avg_cost = train(train_data)\n",
    "    print(f'Epoch {i + 1}/{EPOCHS}, cost: {avg_cost}')\n",
    "\n",
    "end_time = time.time()\n",
    "print(f'Training took {end_time - start_time} seconds')\n",
    "# Save the model\n",
    "np.savez('model.npz', w_1 = w_1, b_1 = b_1, w_2 = w_2, b_2 = b_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
